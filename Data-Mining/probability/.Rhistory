plot(X[,1],X[,2],col=cluster);  		# plot using color for the current clusters
points(proto,pch='x',col=1:K,cex=3);   	 # plot cluster centers as well
readline();					# wait for keyboard input (so we can see what happened so far)
n = 100	   	   # number of points
x = rnorm(100);	   # random x values
y = 5*x + 3 + 2.*rnorm(n)  # random y values with approx linear relation  (rnorm centered around 0)
plot(x,y);    		  # observe
xbar = sum(x)/n;	  # we derived our optimal choice for alpha and beta in terms of these
ybar = sum(y)/n;
xybar = sum(x*y)/n
xsqbar = sum(x*x)/n
b = (ybar*xsqbar-xbar*xybar)/ (xsqbar - xbar*xbar)  # from our calculations
a = (ybar - b)/xbar
abline(b,a)			# add the line that fits the data
cat("a and b are ", a, b, "\n");
n = 100	   	   # number of points
x = rnorm(100);	   # random x values
y = 5*x + 3 + 2.*rnorm(n)  # random y values with approx linear relation  (rnorm centered around 0)
plot(x,y);    		  # observe
xbar = sum(x)/n;	  # we derived our optimal choice for alpha and beta in terms of these
ybar = sum(y)/n;
xybar = sum(x*y)/n
xsqbar = sum(x*x)/n
b = (ybar*xsqbar-xbar*xybar)/ (xsqbar - xbar*xbar)  # from our calculations
a = (ybar - b)/xbar
abline(b,a)			# add the line that fits the data
cat("a and b are ", a, b, "\n");
data(cars)
x = cars$speed
y = cars$dist
n = length(x);
plot(x,y)
xbar = sum(x)/n;	  # this is boilerplate taken from simple_regression.r
ybar = sum(y)/n;
xybar = sum(x*y)/n
xsqbar = sum(x*x)/n
b = (ybar*xsqbar-xbar*xybar)/ (xsqbar - xbar*xbar)  # from our calculations
a = (ybar - b)/xbar
abline(b,a)			# add the line that fits the data
# abline plots line with slope b and y intercept a
cat("a and b are ", a, b, "\n");
data(cars)
x = cars$speed
y = cars$dist
n = length(x);
plot(x,y)
data(attitude)   # R just knows about this dataset
plot(attitude)
# complaints, priveliges, learning, raises, critical, advance
# note "correlations" between some pairs of variables
# cor(attitude) shows correlation matrix between variables
X = as.matrix(attitude[,2:7]);  # X has predictor variables
y = as.vector(attitude[,1]);
a = solve(t(X) %*% X, t(X) %*% y)  # the normal equations in R   %*% is matrix multiplication
cat(a, '\n')    	       	      	       	   # is vector of optimal coefficients for prediction y from x
yhat = X %*% a	 # our predictions of y values
error = y - yhat
sse = sum(error*error)  # sum of squared error.  We chose a by minimizing this.
cat(sse, '\n')
n = nrow(X);
for (d in 1:6) {
X = as.matrix(attitude[,2:(2+d-1)]);
a = solve(t(X) %*% X, t(X) %*% y)
yhat = X %*% a
error = y - yhat
sse = sum(error*error)
cat("with ", d, " variables  got sse of ", sse, "\n");
}
n = 100
x = 2*runif(n);
n = 100
x = 2*runif(n);
#y = 3 + 4*x  + rnorm(n);
y = 3 + 4*x + 5*x^2 + rnorm(n);
plot(x,y);
#X = cbind(rep(1,n),x)		# cbind is "column bind" -- makes matrix out of columns.
X = cbind(rep(1,n),x,x^2)
a = solve(t(X) %*% X, t(X) %*% y)
yhat = X %*% a
lines(sort(x),sort(yhat));
View(X)
erie = read.csv("/monthly-lake-erie-levels-1921-19.csv",stringsAsFactors=FALSE, sep=",");  # really to 1971!
erie = read.csv("polynomial_regression_lake_erie.csv",stringsAsFactors=FALSE, sep=",");  # really to 1971!
erie = read.csv("monthly-lake-erie-levels-1921-19.csv",stringsAsFactors=FALSE, sep=",");  # really to 1971!
# fitting of lake erie data with polynomial regression
# can fit a very high-degree polynomial this way
getwd()
# fitting of lake erie data with polynomial regression
# can fit a very high-degree polynomial this way
setwd("/Users/janechoi/Desktop/IU/DATA MINING/R codes/Regression")
erie = read.csv("monthly-lake-erie-levels-1921-19.csv",stringsAsFactors=FALSE, sep=",");  # really to 1971!
y = erie[1:600,2]
n = length(y);
x = (1:n)/n       # scale to 0 to 1
plot(x,y)
#X = cbind(rep(1,n),x);
X = cbind(rep(1,n),x,x^2,x^3,x^4,x^5);
a = solve(t(X) %*% X, t(X) %*% y);
yhat = X %*% a;
lines(x,yhat,col=2);
error=y-yhat
cat(sum(error^2))
n
x
error=y-yhat
cat(sum(error^2))
x = read.csv2("/Users/yucong/Desktop/B365-Fall2019/Fa2019/data/chilean_voting.csv",stringsAsFactors=FALSE, sep=",")
# first "clean" data to retain only yes voters and no voters
vote = 9;       # the vote (Y,N,A,U) is 9th column
yes = (x[,vote] == "Y" & !(is.na(x[,vote])))   # boolean vector giving "Yes" voters who are not missing
no = (x[,vote] == "N" & !(is.na(x[,vote])))
x = x[no | yes,]
x = x[complete.cases(x), ]
sample.size = nrow(x); nrow(x)
names(x)
sample.index = sample(1:sample.size, 170, replace = FALSE)
test = x[sample.index, ]
x = x[-sample.index, ]
train.size = nrow(x)
################################
x[, 5] = floor(x[, 5]/10)
test[, 5] = floor(test[, 5]/10)
region = 2
population = 3
gender = 4
age = 5
cow = read.csv("/monthly-milk-production-pounds-p.csv",stringsAsFactors=FALSE, sep=",");
cow = read.csv("monthly-milk-production-pounds-p.csv",stringsAsFactors=FALSE, sep=",");
n = 167
y = cow[1:n,2]
plot(y,type="l");
x = 1:n
#X = cbind(rep(1,n),x);
#X = cbind(rep(1,n),x,cos(2*pi*x/12),sin(2*pi*x/12))
X = cbind(rep(1,n),x,x^2,x^3)
X = cbind(rep(1,n),x,x^2,x^3)
a = solve(t(X) %*% X, t(X) %*% y);
yhat = X %*% a;
lines(x,yhat,col=2);
n = 10;
X = matrix(rnorm(n*n),ncol=n);
y = rnorm(n)
a = solve(X,y);
e = y - X %*% a;
cat("sq error is ", sum(e*e), "\n");
n = 100
Z = matrix(rnorm(n*n),nrow=n,ncol=n);
y = rnorm(n)
for (i in 1:n) {
X = Z[,1:i]
a = solve(t(X) %*% X, t(X) %*% y);
yhat = X %*% a
error = y-yhat
sse = sum(error*error)
cat(i, " predictor variables, sse = ", sse, "\n");
}
n = 100
d = 100   # note we have as many predictor variables as observations so likely overfitting
X = matrix(rnorm(n*d),nrow = n, ncol=d);
y = 5*X[,1] + rnorm(n);	   # the relation between the response and predictors only involves 1st predictor variable!
plot(X[,1],y);    # can see obvious relation between first predictor and y
a = solve(t(X) %*% X, t(X) %*% y);
#lambda = .5;      # for ridge regression
#a = solve(t(X) %*% X + lambda*diag(d), t(X) %*% y);   # ridge regression solves different optimization
yhat = X %*% a;
plot(y,yhat);    # perfect prediction.  we should be happy ..... ?
X = matrix(rnorm(n*d),nrow = n, ncol=d);  # try new data from same model
y = 5*X[,1] + rnorm(n);
yhat = X %*% a;
plot(y,yhat);			# prediction not so good ... but improves with ridge regression
#cat("lambda = ", lambda, " test error = ", sum((y-yhat)^2), "normsq of a = ", sum(a*a), "\n")  # for RR
cat(" test error = ", sum((y-yhat)^2), "normsq of a = ", sum(a*a), "\n")
d = 100   # note we have as many predictor variables as observations so likely overfitting
X = matrix(rnorm(n*d),nrow = n, ncol=d);
y = 5*X[,1] + rnorm(n);	   # the relation between the response and predictors only involves 1st predictor variable!
plot(X[,1],y);
a = solve(t(X) %*% X, t(X) %*% y);
#lambda = .5;      # for ridge regression
#a = solve(t(X) %*% X + lambda*diag(d), t(X) %*% y);   # ridge regression solves different optimization
yhat = X %*% a;
plot(y,yhat);    # perfect prediction.  we should be happy ..... ?
X = matrix(rnorm(n*d),nrow = n, ncol=d);  # try new data from same model
y = 5*X[,1] + rnorm(n);
yhat = X %*% a;
plot(y,yhat);			# prediction not so good ... but improves with ridge regression
#cat("lambda = ", lambda, " test error = ", sum((y-yhat)^2), "normsq of a = ", sum(a*a), "\n")  # for RR
cat(" test error = ", sum((y-yhat)^2), "normsq of a = ", sum(a*a), "\n")
n = 100
d = 100   # note we have as many predictor variables as observations so likely overfitting
X = matrix(rnorm(n*d),nrow = n, ncol=d);
y = 5*X[,1] + rnorm(n);	   # the relation between the response and predictors only involves 1st predictor variable!
plot(X[,1],y);    # can see obvious relation between first predictor and y
lambda = .5;      # for ridge regression
a = solve(t(X) %*% X + lambda*diag(d), t(X) %*% y);   # ridge regression solves different optimization
yhat = X %*% a;
plot(y,yhat);    # perfect prediction.  we should be happy ..... ?
X = matrix(rnorm(n*d),nrow = n, ncol=d);  # try new data from same model
y = 5*X[,1] + rnorm(n);
yhat = X %*% a;
plot(y,yhat);			# prediction not so good ... but improves with ridge regression
cat("lambda = ", lambda, " test error = ", sum((y-yhat)^2), "normsq of a = ", sum(a*a), "\n")  # for RR
n = 100
d = 100;
X = matrix(rnorm(n*d),nrow = n, ncol=d);
truea = c(1:5,rep(0,d-5))    	# note that only the first 5 vars are useful with 5th the most useful
y = X %*% truea + rnorm(n);
used = rep(FALSE,d);		# initially all varaibles available for selection
var = rep(0,d);			# var[j] will be variable chosen in jth round
bestsse = rep(10000000,d);      # bestsse[j] will be best sse from jth round
for (j in 1:d)  {		# choose 1 variable each time through this loop
for (i in which(used == FALSE)) {
used[i] = TRUE;
XX = X[,used];		# take the "used" columns = used variables
a = solve(t(XX) %*% XX , t(XX) %*% y);
yhat = XX %*% a;
error = y-yhat;
sse = sum(error*error);
if (sse < bestsse[j]) {	# if we find a better sse, take it
bestsse[j] = sse;
var[j] = i;
}
used[i] = FALSE;
}
used[var[j]] = TRUE;	 # claim the best variable for future iterations of loop
cat(var[j],'\n')
}
plot(bestsse)
n = 100
d = 100;
X = matrix(rnorm(n*d),nrow = n, ncol=d);
truea = c(1:5,rep(0,d-5))
y = X %*% truea + rnorm(n);
used = rep(FALSE,d);		# initially all varaibles available for selection
var = rep(0,d);			# var[j] will be variable chosen in jth round
bestsse = rep(10000000,d);      # bestsse[j] will be best sse from jth round
for (j in 1:d)  {		# choose 1 variable each time through this loop
for (i in which(used == FALSE)) {
used[i] = TRUE;
XX = X[,used];		# take the "used" columns = used variables
a = solve(t(XX) %*% XX , t(XX) %*% y);
yhat = XX %*% a;
error = y-yhat;
sse = sum(error*error);
if (sse < bestsse[j]) {	# if we find a better sse, take it
bestsse[j] = sse;
var[j] = i;
}
used[i] = FALSE;
}
used[var[j]] = TRUE;	 # claim the best variable for future iterations of loop
cat(var[j],'\n')
}
plot(bestsse)
var
knitr::opts_chunk$set(echo = TRUE)
used = rep(TRUE,d)
var = rep(0,d)			# var[j] will be variable chosen in jth round
sseplot = rep(10000,(d-1))      # sseplot[j] will be best sse from jth round
for (j in 1:(d-1)) {		# choose 1 variable each time through this loop
for (i in which(used == TRUE)) {
used[i] = FALSE
XX = X[,used]		# take the "used" columns = used variables
a = solve(t(XX) %*% XX , t(XX) %*% Y)
yhat = XX %*% a
error = Y-yhat
sse = sum(error*error)
if (sse < sseplot[j]) {	# if we find a better sse, take it
sseplot[j] = sse
var[j] = i
}
used[i] = TRUE
}
used[var[j]] = FALSE	 # claim the best variable for future iterations of loop
cat('The eliminated variable is',var[j], ', The SSE right now is', sseplot[j], '\n')
}
set.seed(123)
n = 10
d = 10
X = matrix(rnorm(n*d),nrow = n, ncol=d)
truea = c(1:5,rep(0,d-5))    # first 5 vars are useful with 5th the most useful
Y = X %*% truea + rnorm(n)
used = rep(TRUE,d)
var = rep(0,d)			# var[j] will be variable chosen in jth round
sseplot = rep(10000,(d-1))      # sseplot[j] will be best sse from jth round
for (j in 1:(d-1)) {		# choose 1 variable each time through this loop
for (i in which(used == TRUE)) {
used[i] = FALSE
XX = X[,used]		# take the "used" columns = used variables
a = solve(t(XX) %*% XX , t(XX) %*% Y)
yhat = XX %*% a
error = Y-yhat
sse = sum(error*error)
if (sse < sseplot[j]) {	# if we find a better sse, take it
sseplot[j] = sse
var[j] = i
}
used[i] = TRUE
}
used[var[j]] = FALSE	 # claim the best variable for future iterations of loop
cat('The eliminated variable is',var[j], ', The SSE right now is', sseplot[j], '\n')
}
plot(sseplot, main= 'Plot of Square Error Change for Backward Selection', xlab ='Eliminated variable number', ylab='Square error')
N = 100000
D = rep(FALSE,N);
T = rep(FALSE,N);
for (i in 1:N) {
D[i] = (runif(1) < .001)  # D is boolean variable for disease
if (D[i]) {
T[i] = (runif(1) < .99);
}
else {
T[i] = (runif(1) < .01)
}
}
cat("estimate of P(D | +) = ", sum(D&T) / sum(T) , "\n");
data(Titanic)    # R just knows about these data
print(dimnames(Titanic)) # the variables and their categorizations
print(Titanic)   # R plots all possible tables for each *fixed* value of last two variables
print(Titanic["1st","Female",,])   # table of 1st class female by Age and Survival  (conditioning)
print(apply(Titanic, c("Sex","Survived"),sum));  # table on  Sex and Survived
print(apply(Titanic["1st",,,],c("Sex","Survived"),sum))   # table on Sex and Survived for the 1st class
mosaicplot(apply(Titanic, c("Class","Survived"),sum));  # table on  Sex and Survived
mosaicplot(apply(Titanic, c("Class","Sex"),sum));  # table on  Sex and Survived
mosaicplot(apply(Titanic, c("Class","Survived"),sum));  # table on  Sex and Survived
mosaicplot(apply(Titanic, c("Class","Sex"),sum));  # table on  Sex and Survived
stopp;
mosaicplot(apply(Titanic[,"Female",,], c("Class","Survived"),sum));  # table on  Sex and Survived
# But note that "Female" is more heavily weighted in the higher classes.
# But note that "Female" is more heavily weighted in the higher classes.
# Could it be that "1st"-class passengers just appear to have been favored because they
# But note that "Female" is more heavily weighted in the higher classes.
# Could it be that "1st"-class passengers just appear to have been favored because they
# contained more Females?
# But note that "Female" is more heavily weighted in the higher classes.
# Could it be that "1st"-class passengers just appear to have been favored because they
# contained more Females?
# To consider this, look at the 2-way "Class"x"Survived table separately for both "Female" and "Male".
# But note that "Female" is more heavily weighted in the higher classes.
# Could it be that "1st"-class passengers just appear to have been favored because they
# contained more Females?
# To consider this, look at the 2-way "Class"x"Survived table separately for both "Female" and "Male".
# Even doing this it appears that "1st" class was was prefered:
x = read.csv2("./chilean_voting.csv",stringsAsFactors=FALSE,sep=",");
# data on Chilean election for Pinochet in 1988.
getwd()
# data on Chilean election for Pinochet in 1988.
setwd( "/Users/janechoi/Desktop/IU/DATA MINING/R codes/probability")
x = read.csv2("./chilean_voting.csv",stringsAsFactors=FALSE,sep=",");
vote = 9;        # the vote (Y,N,A,U) is 9th column
education = 6;   # education is the 6th column
gender = 4;
region = 2;
yes = (x[,vote] == "Y" & !(is.na(x[,vote])))   # boolean vector giving "Yes" voters who are not missing
no = (x[,vote] == "N" & !(is.na(x[,vote])))
x = x[no | yes,]
#t = table(x[,c(region,vote)])
#t = table(x[,c(education,vote)])
t = table(x[,c(education,gender,vote)])
#t = table(x[,c(education,gender,region,vote)])
print(t)
mosaicplot(t)
cond_prob = prop.table(t,1);
cond_prob
n = 1000    # number of times we simulate
x = rep(0,n);  # will hold the results
for (i in 1:n) {
x[i] =  sum(runif(3) < .5)
}
print(table(x)/n)  # our estimated probability distribution.
table(x)
data(iris3)   # (different form of same data) 3 dim array,  iris3[50 examples, 4 attributes, 3 classes]
petal_len = 3
bk = c(0,.5,1.,1.5,2.,8)  # the breakpoints we will used in quantizing the petal_len variable
values = length(bk)-1;
iris3[,petal_len,] = cut(iris3[,petal_len,],breaks=bk,labels=1:values)   # substitute the  quantized values
x = iris3[,petal_len,];    # just to simplify going forward ...  x[i,c] is the quantized value
x
Q = array(0,dim=c(3,values));	# Q[c,v] is P(x = v | class c)
Q
x
for (c in 1:3) {
for (v in 1:values) {
Q[c,v] = sum(x[,c] == v)/50   # estimate probs by usual counting
}
}
print(Q)
prior = rep(1/3,3);  # prior not very important in this example since data chosen to have 50 examples from each class
prior = rep(1/3,3);  # prior not very important in this example since data chosen to have 50 examples from each class
prior = rep(1/3,3);  # prior not very important in this example since data chosen to have 50 examples from each class
prior = rep(1/3,3);  # prior not very important in this example since data chosen to have 50 examples from each class
error = 0;     # will count the # of errors our Bayes classifier makes
for (c in 1:3) {   # for each class
for (i in 1:5) {    # for each flower ...
p = prior;
v = x[i,c];
p = p*Q[,v];
p = p/sum(p)  # make them into probs
cat("true class: ",c,"nb says: ",which.max(p),"\n");
if (c != which.max(p)) error = error+1;
}
}
cat("errors = ", error, "\n");
x
Q[,v]
print(Q)
data(iris3)   # (different form of same data) 3 dim array,  iris3[50 examples, 4 attributes, 3 classes]
petal_len = 3
bk = c(0,.5,1.,1.5,2.,8)  # the breakpoints we will used in quantizing the petal_len variable
values = length(bk)-1;
iris3[,petal_len,] = cut(iris3[,petal_len,],breaks=bk,labels=1:values)   # substitute the  quantized values
x = iris3[,petal_len,];    # just to simplify going forward ...  x[i,c] is the quantized value
# of ith flower from cth class
Q = array(0,dim=c(3,values));	# Q[c,v] is P(x = v | class c)
Q
source('~/Desktop/IU/DATA MINING/R codes/probability/iris_one_feature_bayes_class.r', echo=TRUE)
x
Q = array(0,dim=c(3,values));	# Q[c,v] is P(x = v | class c)
data(iris3)   # (different form of same data) 3 dim array,  iris3[50 examples, 4 attributes, 3 classes]
petal_len = 3
bk = c(0,.5,1.,1.5,2.,8)  # the breakpoints we will used in quantizing the petal_len variable
values = length(bk)-1;
iris3[,petal_len,] = cut(iris3[,petal_len,],breaks=bk,labels=1:values)   # substitute the  quantized values
x = iris3[,petal_len,];    # just to simplify going forward ...  x[i,c] is the quantized value
# of ith flower from cth class
Q = array(0,dim=c(3,values))
x
Q
for (c in 1:3) {
for (v in 1:values) {
Q[c,v] = sum(x[,c] == v)/50   # estimate probs by usual counting
}
}
print(Q)
prior = rep(1/3,3);  # prior not very important in this example since data chosen to have 50 examples from each class
error = 0;     # will count the # of errors our Bayes classifier makes
x
for (i in 1:50) {    # for each flower ...
p = prior;
v = x[i,c];
p = p*Q[,v];
p = p/sum(p)  # make them into probs
cat("true class: ",c,"nb says: ",which.max(p),"\n");
if (c != which.max(p)) error = error+1;
}
for (c in 1:3) {   # for each class
for (i in 1:50) {    # for each flower ...
p = prior;
v = x[i,c];
p = p*Q[,v];
p = p/sum(p)  # make them into probs
cat("true class: ",c,"nb says: ",which.max(p),"\n");
if (c != which.max(p)) error = error+1;
}
}
cat("errors = ", error, "\n");
print(Q)
x
p
which.max(p)
cat("true class: ",c,"nb says: ",which.max(p),"\n");
Q[,v]
data(iris3)   # (different form of same data) 3 dim array,  iris3[50 examples, 4 attributes, 3 classes]
for (f in 1:4) {    	    # for each feature
bk = quantile(iris3[,f,],c(0,.25,.5,.75,1))  # the 0%, 25% , 50%, 75%, 100% quantiles
bk[1] = 0;            # kludge for endpoint problem
iris3[,f,] = cut(iris3[,f,],breaks=bk,labels=1:4)   # substitute the 4 quantized values
}
Q = array(0,dim=c(3,4,4));	# Q[c,f,v] is P(feature f = v | class c)
Q = array(0,dim=c(3,4,4));	# Q[c,f,v] is P(feature f = v | class c)
Q
iris3
Q = array(0,dim=c(3,4,4));	# Q[c,f,v] is P(feature f = v | class c)
for (c in 1:3) {
for (f in 1:4) {
for (v in 1:4) {
Q[c,f,v] = sum(iris3[,f,c] == v)/50   # estimate probs by usual counting
}
}
}
Q
Q
prior = rep(1/3,3);  # prior not very important in this example since data chosen to have 5 examples from each class
iris3
Q
data(iris)   # (different form of same data) 3 dim array,  iris3[50 examples, 4 attributes, 3 classes]
d = as.matrix(dist(iris[,1:4]))
n = nrow(iris);
class = as.numeric(iris[,5])
classhat = rep(0,n);
for (i in 1:n) {
j = which.min(d[i,(1:n) != i])
classhat[i] = iris[j,5];
}
cat ("error rate = ", sum(class != classhat)/n,"\n");
d = as.matrix(dist(iris[,1:4]))
data(iris)   # (different form of same data) 3 dim array,  iris3[50 examples, 4 attributes, 3 classes]
d = as.matrix(dist(iris[,1:4]))
d
n = nrow(iris);
class = as.numeric(iris[,5])
iris[,1:4]
library(rpart);
pairs(stagec);
progstat = factor(stagec$pgstat, levels = 0:1, labels = c("No", "Prog"))
plot(stagec$ploidy,progstat)
plot(progstat,stagec$gleason)
plot(progstat,stagec$g2)
eet = factor(stagec$eet,levels=1:2,labels=c("No","Yes"))
mosaicplot(table(eet,progstat))
fit  = rpart(progstat ~  age + eet + g2 + grade + gleason + ploidy, data = stagec, method ="class",parms = list(split = 'information'))
#fit  = rpart(progstat ~  age + eet + g2 + grade + gleason + ploidy, data = stagec, method ="class",parms = list(split = 'gini'))
print(fit)
