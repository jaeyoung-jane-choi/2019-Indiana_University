x = read.csv2("./chilean_voting.csv",stringsAsFactors=FALSE,sep=",");
vote = 9;        # the vote (Y,N,A,U) is 9th column
education = 6;   # education is the 6th column
gender = 4;
region = 2;
yes = (x[,vote] == "Y" & !(is.na(x[,vote])))   # boolean vector giving "Yes" voters who are not missing
no = (x[,vote] == "N" & !(is.na(x[,vote])))
x = x[no | yes,]
#t = table(x[,c(region,vote)])
#t = table(x[,c(education,vote)])
t = table(x[,c(education,gender,vote)])
#t = table(x[,c(education,gender,region,vote)])
print(t)
t['P',,]
0
sum(t['P',,]
sum(t['P',,])
colSums(t['P',,])
(t['P','F','N'])
knitr::opts_chunk$set(echo = TRUE)
setwd("/Users/janechoi/Desktop/IU/DATA/datamining/HW4")
library(tidyverse)
x = read.csv2("./chilean_voting.csv",stringsAsFactors=FALSE,sep=",")
vote = 9
education = 6
gender = 4
region = 2
age= 5
yes = (x[,vote] == "Y" & !(is.na(x[,vote])))
no = (x[,vote] == "N" & !(is.na(x[,vote])))
x = x[no | yes,]
x[,5] = floor(x[,5]/10)
t = table(x[,c(age,education,vote)])
t
vote.y = sum(x$vote == 'Y') / nrow(x)
vote.n = sum(x$vote == 'N') /nrow(x)
cat('The prior distribution of vote is yes is : ', vote.y ,'\n', 'The prior distribution of vote is no is: ',
vote.n)
Q = array(0,dim=c(7,3,2))	# Q[c,v] is P(x = v | class c)
for (a in 1:7){
for (e in 1:3){
for ( v in 1:2){
if (v == 1){
Q[a,e,v]= t[a,e,1] /( t[a,e,1]  +t[a,e,2] )
}else{
Q[a,e,v]= t[a,e,2] /( t[a,e,1]  +t[a,e,2] )
}
}
}
}
Q
data =(x[,c(age,education,vote)])
prior = c(vote.n, vote.y)
t = table(x[,c(gender,education,region,age,vote)])
vote.Y = t['F','PS','SA',4,'N']/ (t['F','PS','SA',4,'N'] +t['F','PS','SA',4,'Y'])
vote.Y = t['F','PS','SA',4,'N']/ (t['F','PS','SA',4,'N'] +t['F','PS','SA',4,'Y'])
vote.Y = t['F','PS','SA',4,'N']/ (t['F','PS','SA',4,'N'] +t['F','PS','SA',4,'Y'])
vote.N =t['F','PS','SA',4,'Y']/ (t['F','PS','SA',4,'N'] +t['F','PS','SA',4,'Y'])
vote.N = t['F','PS','SA',4,'N']/ (t['F','PS','SA',4,'N'] +t['F','PS','SA',4,'Y'])
vote.Y =t['F','PS','SA',4,'Y']/ (t['F','PS','SA',4,'N'] +t['F','PS','SA',4,'Y'])
cat('By the Bayes classifier, a female, post-secondary-educated, from  SA region, in her 40s would be classified as : "yes",\n since the probability for yes is ', vote.Y, 'and the probability for no is ', vote.N)
t = table(x[,c(gender,education,region,age,vote)])
vote.N = t['F','PS','SA',4,'N']/ (t['F','PS','SA',4,'N'] +t['F','PS','SA',4,'Y'])
vote.Y =t['F','PS','SA',4,'Y']/ (t['F','PS','SA',4,'N'] +t['F','PS','SA',4,'Y'])
cat('By the Bayes classifier, a female, post-secondary-educated, from  SA region, in her 40s would be classified as : "no",\n since the probability for yes is ', vote.Y, 'and the probability for no is ', vote.N)
vote.N = t['F','PS','SA',4,'N']/ (t['F','PS','SA',4,'N'] +t['F','PS','SA',4,'Y'])*prior[1] #no
vote.Y =t['F','PS','SA',4,'Y']/ (t['F','PS','SA',4,'N'] +t['F','PS','SA',4,'Y'])*prior[2]
cat('By the Bayes classifier, a female, post-secondary-educated, from  SA region, in her 40s would be classified as : "no",\n since the probability for yes is ', vote.Y, 'and the probability for no is ', vote.N)
data =(x[,c(age,education,vote)])
prior = c(vote.n, vote.y)
#P , PS ,S
data$education<-as.factor(data$education)
levels(data$education) = c(1,2,3)
chat = array(0,dim=c(7,3))
p =array(0,2)
for (a in 1:7) {   # for each class
for (e in 1:3) {
for ( v in 1:2){
if (v ==1 ){ #when vote = N
p[1] = prior[1]*sum(Q[a,e,1])
}else{ #when vote = Y
p[2] = prior[2]*sum(Q[a,e,2])
}
}
chat[a,e] = which.max(p)
}
}
chat
t = table(x[,c(gender,education,region,age,vote)])
vote.N = t['F','PS','SA',4,'N']/ (t['F','PS','SA',4,'N'] +t['F','PS','SA',4,'Y'])*prior[1] #no
vote.Y =t['F','PS','SA',4,'Y']/ (t['F','PS','SA',4,'N'] +t['F','PS','SA',4,'Y'])*prior[2]
cat('By the Bayes classifier, a female, post-secondary-educated, from  SA region, in her 40s would be classified as : "no",\n since the probability for yes is ', vote.Y, 'and the probability for no is ', vote.N)
sum(Q[a,e,1])
(Q[a,e,1])
data =(x[,c(age,education,vote)])
prior = c(vote.n, vote.y)
#P , PS ,S
data$education<-as.factor(data$education)
levels(data$education) = c(1,2,3)
chat = array(0,dim=c(7,3))
p =array(0,2)
for (a in 1:7) {   # for each class
for (e in 1:3) {
for ( v in 1:2){
if (v ==1 ){ #when vote = N
p[1] = prior[1]*(Q[a,e,1])
}else{ #when vote = Y
p[2] = prior[2]*(Q[a,e,2])
}
}
chat[a,e] = which.max(p)
}
}
chat
num.N= t['F','PS','SA',4,'N']
num.Y =t['F','PS','SA',4,'Y']
cat("For the Bayes' classifier , it requires a larger number of records for the errors to decrease. However in this case each record number is:", num.N, 'and', num.Y , 'which are not enough for precise calculation for bayes' )
data(iris3)   # (different form of same data) 3 dim array,  iris3[50 examples, 4 attributes, 3 classes]
for (f in 1:4) {    	    # for each feature
bk = quantile(iris3[,f,],c(0,.25,.5,.75,1))  # the 0%, 25% , 50%, 75%, 100% quantiles
bk[1] = 0;            # kludge for endpoint problem
iris3[,f,] = cut(iris3[,f,],breaks=bk,labels=1:4)   # substitute the 4 quantized values
}
Q = array(0,dim=c(3,4,4));	# Q[c,f,v] is P(feature f = v | class c)
for (c in 1:3) {
for (f in 1:4) {
for (v in 1:4) {
Q[c,f,v] = sum(iris3[,f,c] == v)/50   # estimate probs by usual counting
}
}
}
Q
prior = rep(1/3,3);  # prior not very important in this example since data chosen to have 5 examples from each class
